<configuration>
  <!--指定块的副本数，默认是3-->
  <property>
    <name>dfs.replication</name>
    <value>2</value>
  </property>
  
  <!--指定数据块的大小-->
  <property>
    <name>dfs.blocksize</name>
    <value>134217728</value>
  </property>
  
  <!--指定namenode的元数据目录-->
  <property>
    <name>dfs.namenode.name.dir</name>
    <value>/home/{{ admin_user }}/server/hadoop/data/namenode</value>
  </property>
  
  <!--指定datanode存储数据目录-->
  <property>
    <name>dfs.datanode.data.dir</name>
    <value>/home/{{ admin_user }}/server/hadoop/data/datanode</value>
  </property>
  
  <!--hdfs的命名空间,逻辑名称-->
  <property>
    <name>dfs.nameservices</name>
    <value>admin</value>
  </property>
  
  {% if hdfs_ha_tag %}
  <!--namenode的别名-->
  <property>
    <name>dfs.ha.namenodes.admin</name>
    <!-- <value>nn1,nn2</value> -->
    <value>{% for i in groups['namenode'] %}{% if loop.index0 <= 3 %}nn{{ loop.index }}{%- if not loop.last %},{% endif -%}{% endif %}{% endfor %}</value>
  </property>
  
  <!--指定nn1和nn2的通信地址,如果namenode多个，则这里也会有多个-->
  {% for ip in groups['namenode'] %}
  {% if loop.index0 <= 3 %}
  <property>
    <name>dfs.namenode.rpc-address.admin.nn{{ loop.index }}</name>
    <value>{{ hostvars[ip]['ansible_hostname'] }}:{{ hdfs_rpc_port }}</value>
  </property>
  <!--指定namenode的web通信地址-->
  <property>
    <name>dfs.namenode.http-address.admin.nn{{ loop.index }}</name>
    <value>{{ hostvars[ip]['ansible_hostname'] }}:{{ hdfs_namenode_httpaddress_port }}</value>
  </property>
  {% endif %}
  {% endfor %}
  <!--指定共享日志目录-->
  <property>
    <name>dfs.namenode.shared.edits.dir</name>
    <value>qjournal://{% for i in groups['datanode'] %}{% if loop.index0 <= 2 %}{{ hostvars[i]['ansible_hostname'] }}:{{ hdfs_namenode_shared }}{%- if not loop.last %};{% endif -%}{% endif %}{% endfor %}/admin</value>

  </property>
  
  <!--指定开启namenode失败自动转移-->
  <property>
     <name>dfs.ha.automatic-failover.enabled</name>
     <value>true</value>
  </property>
  
  <!--指定失败转移的类-->
  <property>
    <name>dfs.client.failover.proxy.provider.admin</name>
    <value>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider</value>
  </property>
  
  
  <!--防止namenode的脑裂-->
  <property>
     <name>dfs.ha.fencing.methods</name>
     <value>sshfence</value>
  </property>
  
  <property>
     <name>dfs.ha.fencing.ssh.private-key-files</name>
     <value>/home/{{ admin_user }}/.ssh/id_rsa</value>
  </property>
  
  <!--指定超时时间设置-->
  <property>
    <name>dfs.ha.fencing.ssh.connect-timeout</name>
    <value>30000</value>
  </property>

  {% else %}
  <property>
    <name>dfs.namenode.http-address</name>
    <value>{{ hostvars[groups['namenode'][0]]['ansible_hostname'] }}:{{ hdfs_namenode_httpaddress_port }}</value>
  </property>
  <property>
    <name>dfs.namenode.secondary.http-address</name>
    <value>{{ hostvars[groups['namenode'][0]]['ansible_hostname'] }}:{{ hdfs_namenode_secondary_httpaddress_port }}</value>
  </property>
  {% endif %}
  <!--指定日志的本地目录-->
  <property>
    <name>dfs.journalnode.edits.dir</name>
    <value>/home/{{ admin_user }}/server/hadoop/data/journal</value>
  </property>
  
  <!--是否开启webhdfs的-->
  <property>
    <name>dfs.webhdfs.enabled</name>
    <value>true</value>
  </property>
  
  <!--是否开启hdfs的权限-->
  <property>
    <name>dfs.permissions</name>
    <value>false</value>
  </property>
  <property>
    <name>fs.trash.interval</name>
    <value>120</value>
  </property>
  <property>
    <name>fs.trash.checkpoint.interval</name>
    <value>120</value>
  </property>
  
</configuration>